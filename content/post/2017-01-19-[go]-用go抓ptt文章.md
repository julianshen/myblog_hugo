---
date: "2017-01-19T11:45:30Z"
images:
- /images/posts/2017-01-19-[go]-用go抓ptt文章.md.jpg
tags:
- golang
- ptt
- crawler
- bot
title: '[Go] 用Go抓PTT文章'
---

好, 這算我以為我寫過但實際上沒有系列....啥? 剛剛把我以前寫的一個[Go package - gopttcrawler](https://github.com/julianshen/gopttcrawler)更新後,
想說之前好像有寫過相關文章, 但實際上又沒找到(老了?)

沒關係, 把Readme拿來貼一貼就可以混一篇文了(混蛋!!!偷懶!!!)

[gopttcrawler](https://github.com/julianshen/gopttcrawler)

這是我在新聞萬事通裡面用來抓取ptt文章的, 用法也很簡單, 基本上看最後兩種用法就好, 自己個人覺得那比較好用

原本的版本並沒處理18+那個擋在前面的畫面, 查了一下很多人(在python)的做法是去發post取得cookie, 再帶cookie去取, 後來發現, cookie是固定的, 只要這樣就好:

```go
	cookie := http.Cookie{
		Name:  "over18",
		Value: "1",
	}
	req.AddCookie(&cookie)
```

所以現在像是八卦版這種18+的也可以抓取了

安裝方法: `go get -u github.com/julianshen/gopttcrawler`

使用方法請參考sample.go或是ptt_test.go
#### 資料結構

```go
type Article struct {
	ID       string //Article ID
	Board    string //Board name
	Title    string
	Content  string
	Author   string //Author ID
	DateTime string
	Nrec     int //推文數(推-噓)
}

type ArticleList struct {
	Articles     []*Article //Articles
	Board        string //Board
	PreviousPage int //Previous page id
	NextPage     int //Next page id
}
```

#### 載入文章列表
1. 載入最新一頁表特版文章

```go
    articleList, _ := gopttcrawler.GetArticles("Beauty", 0)
    // the 1st parameter is the board name
    // the 2nd parameter is the page id. 0 indicates the latest page
```

2. 載入前一頁文章列表

```go
    prevArticleList, _ := articleList.GetFromPreviousPage()
```

#### 載入文章內容
1. 載入單篇文章詳細內容

```go
    article := articleList.Articles[0]
    article.Load()
    fmt.Println(article.Content) //印出內文(html)
```

2. 取得文章中所有圖片連結

```go
    images := article.GetImageUrls()
```

3. 取得文章中的連結

```go
    links := article.GetLinks()
```

#### Iterator

新增Iterator功能:

```go
	n := 100

	articles, e := gopttcrawler.GetArticles("movie", 0)
	
	if e != nil {
		....
	}

	iterator := articles.Iterator()

	i := 0
	for {
		if article, e := iterator.Next(); e == nil {
			if i >= n {
				break
			}
			i++

			log.Printf("%v %v", i, article)
		}
	}
```

上面這範例是抓取最新的100篇文章, 不用管第幾頁, 或是上一頁下一頁, 反正就一直抓

#### Go routine版本的GetArticles

```go
	ch, done := gopttcrawler.GetArticlesGo("Beauty", 0)
	n := 100
	i := 0
	for article := range ch {
		if i >= n {
			done <- true
			break
		}
		i++
		log.Printf("%v %v", i, article)
	}
```

這範例一樣也是抓一百篇, 只是抓文章的部分被放到go routine去了, 會立即回傳兩個channel,
第一個是receive only channel, 跟Iterator類似, 一次拿一篇文章, 可以用range, 第二個是一個bool channel, 拿夠了送個訊息通知他終止go routine,
如果把receive部分放到select去, 就是non blocking了, 不會被讀上一頁下一頁的IO給卡住